\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{biblatex}
\addbibresource{boatpaper}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{url}
\usepackage[bookmarks]{hyperref}

\setcounter{tocdepth}{3}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\pdc{prior-data conflict}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bm #1}}

\newcommand{\btheta}{\bs{\theta}}
\newcommand{\bpsi}{\bs{\psi}}
\newcommand{\bbeta}{\bs{\beta}}
\newcommand{\balpha}{\bs{\alpha}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i
\newcommand{\uinfty}{^{(\infty)}} % upper infty

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\def\yz{y\uz}
\def\yn{y\un}
\def\yi{y\ui}

\def\yzl{\ul{y}\uz}
\def\yzu{\ol{y}\uz}

\def\ynl{\ul{y}\un}
\def\ynu{\ol{y}\un}

\def\yil{\ul{y}\ui}
\def\yiu{\ol{y}\ui}

\def\nz{n\uz}
\def\nn{n\un}
\def\ni{n\ui}

\def\nzl{\ul{n}\uz}
\def\nzu{\ol{n}\uz}

\def\nnl{\ul{n}\un}
\def\nnu{\ol{n}\un}

\def\nil{\ul{n}\ui}
\def\niu{\ol{n}\ui}

\def\taux{\tau(x)}
\def\ttau{\tilde{\tau}}
\def\ttaux{\ttau(x)}

\def\pz{\psi\uz}
\def\pn{\psi\un}

\def\Y{\mathcal{Y}}
\def\YZ{\Y\uz}
\def\YN{\Y\un}
\def\YI{\Y\ui}
\def\N{\mathcal{N}}
\def\NZ{\N\uz}
\def\NN{\N\un}
%\def\PZ{\text{I}\!\Pi\uz}
\def\PZ{I\!\!\Pi\uz}
%\def\PN{\text{I}\!\Pi\un}
\def\PN{I\!\!\Pi\un}
%\def\Pinfty{\text{I}\!\Pi\uinfty}
\def\MZ{\mathcal{M}\uz}
\def\MN{\mathcal{M}\un}

\newcommand{\az}{\alpha\uz}
\newcommand{\an}{\alpha\un}
\newcommand{\bz}{\beta\uz}
\newcommand{\bn}{\beta\un}

\def\Eta{\mathrm{H}}
\def\EZ{\mathrm{H}\uz}
\def\EN{\mathrm{H}\un}

\def\ezl{\ul{\eta}_0}
\def\ezu{\ol{\eta}_0}

\def\ezz{\eta_0\uz}
\def\ezn{\eta_0\un}
\def\eoz{\eta_1\uz}
\def\eon{\eta_1\un}

\def\ezzl{\ul{\eta}_0\uz}
\def\ezzu{\ol{\eta}_0\uz}
\def\eznl{\ul{\eta}_0\un}
\def\ezzn{\ol{\eta}_0\un}

\def\eozl{\ul{\eta}_1\uz}
\def\eozu{\ol{\eta}_1\uz}
\def\eonl{\ul{\eta}_1\un}
\def\eonu{\ol{\eta}_1\un}

\def\eol{\ul{\eta}_1}
\def\eou{\ol{\eta}_1}

\def\czl{\ul{c}\uz}
\def\czu{\ol{c}\uz}

\newcommand{\cdf}{\operatorname{F}}
\newcommand{\p}{\operatorname{P}}
\newcommand{\q}{\operatorname{Q}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\med}{\operatorname{med}} % Median
\newcommand{\modus}{\operatorname{mode}} % Mode
\newcommand{\logit}{\operatorname{logit}} % logit

%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}

\def\El{\ul{\E}}
\def\Eu{\ol{\E}}

\def\Pl{\ul{\p}}
\def\Pu{\ol{\p}}

\newcommand{\ber}{\operatorname{Ber}}   % Bernoulli Distribution
\newcommand{\bin}{\operatorname{Binom}} % Binomial Distribution
\newcommand{\be}{\operatorname{Beta}}   % Beta Distribution
\newcommand{\B}{\operatorname{B}}   % Beta Function



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Manuscript body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Sets of Prior Distributions\\ for Reflecting Prior-Data Conflict\\ and Strong Prior-Data Agreement}

% a short form should be given in case it is too long for the running head
\titlerunning{Sets of Priors for Reflecting Prior-Data Conflict and Agreement}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Gero Walter\inst{1}%
\thanks{Gero Walter was supported by the Dinalog project
``Coordinated Advanced Maintenance and Logistics Planning for the Process Industries'' (CAMPI).}%
\and Frank P.A.\ Coolen\inst{2}}
%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{%
School of Industrial Engineering,\\
Eindhoven University of Technology, Eindhoven, NL\\
\url{g.m.walter@tue.nl}
\and
Department of Mathematical Sciences,\\
Durham University, Durham, UK\\
\url{frank.coolen@durham.ac.uk}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\maketitle


\begin{abstract}
Bayesian inference allows to combine observations with (possibly subjective) prior knowledge in the reasoning process.
The choice of a particular prior distribution to represent the available prior knowledge is, however,
often debatable, especially when prior knowledge is limited or data are scarce,
as then posterior inferences are higly dependent on the choice of prior.
Robust Bayesian analysis accounts for this issue by
inquiring whether posterior inferences change substantially
when the prior distribution is varied within a set of distributions that contains all `reasonable' priors.
Similar, but slightly different in scope, is the imprecise probability approach,
formalising the idea that sets of probability distributions
%(or, equivalently, interval-valued previsions or sets of desirable gambles)
should be taken to model prior knowledge more acurrately.
With imprecise probability, modeling prior-data conflict sensitivity is possible:
Models have been proposed where the ranges of posterior inferences
are substantially larger when prior and data are in conflict.
Here we propose a new method for generating parameter prior sets in a conjugate setting
that, in addition to prior-data conflict sensitivity, allow to mirror \emph{strong prior-data agreement},
i.e., the case when prior and data coincide especially well, by increased posterior precision.
Although presented here for the case of binary data only,
it is easily extensible to the general exponential family case.
\keywords{Bayesian inference, strong prior-data agreement, prior-data conflict, imprecise probability, conjugate priors}
\end{abstract}


\section{Introduction}

The Bayesian approach to inference \cite[see, e.g.,][]{2007:robert,2005:ruggeri} 
offers the advantage to combine data and prior knowledge in a unified reasoning process.
The prior knowledge, i.e., information extraneous to data, is usually provided in the form of expert knowledge.

Bayesian inference usually starts with defining a parametric \emph{sample model},
denoted by $f(\vec{x} \mid \vartheta)$,
a conditional distribution of data $\vec{x} = (x_1, \ldots, x_n)$ given the value for a parameter $\vartheta$.
Prior information is then encoded by a so-called \emph{prior distribution} $p(\vartheta)$,
expressing the expert's opinion on probable values of $\vartheta$.
In light of the sample $\vec{x}$, the prior distribution is updated by Bayes' Rule
to obtain the so-called \emph{posterior distribution}
\begin{align}
\label{eq:bayesrule}
p(\vartheta\mid\vec{x}) &= \frac{f(\vec{x}\mid\vartheta) f(\vartheta)}{\int f(\vec{x}\mid\vartheta) f(\vartheta) \dd \vartheta}
                         \propto f(\vec{x}\mid\vartheta) \cdot p(\vartheta)\,.
\end{align}
The posterior distribution is understood to comprise all information from the sample and the prior knowledge.
It therefore underlies all further inferences on the parameter $\vartheta$,
like point estimators, interval estimators,
or the \emph{posterior predictive distribution},
giving the distribution of further observations based on the posterior.

However, the choice for the prior distribution to encode the given expert knowledge is often debatable,
and a specific choice of prior is difficult to justify.
A way to deal with this is to employ sensitivity analysis,
i.e., studying the effect of different choices of prior distribution on the quantities of interest.
This idea has been explored in systematic sensitivity analysis, or robust Bayesian methods;
for an overview on this approach, see, e.g.,
\cite{1994:berger} or \cite{2000:rios}. %\citeNP{2005:ruggeri}, \citeNP{2000:bergerinsuaruggeri}

The work we present here can be seen as belonging to the robust Bayesian approach, as our work uses sets of priors.
However, our focus and interpretation is slightly different,
as we consider the result of our procedure, sets of posterior distributions, as the proper result,
while a robust Bayesian would base his analyses on a single posterior from the set
in case (s)he was able to conclude that quantities of interest are not `too sensitive' to the choice of prior.
In contrast, our viewpoint is rooted in the theory of imprecise or interval probability \cite{itip,1991:walley},
where sets of distributions are used to express the precision of probability statements themselves:
the smaller the set of posteriors, the more precise the probability statements.

A central idea in imprecise probability is that the magnitude of a set of distributions,
and, in turn, the width of probability ranges based on it,
should reflect the precision of probabilistic knowledge.
A situation where this relation should especially hold is that of \emph{\pdc}:
% from abstract esrelpaper
%A problem that can arise in Bayesian inference is called prior-data conflict:
From the viewpoint of the prior $p(\vartheta)$, the observed data $\vec{x}$ seem very surprising,
i.e., information from data is in conflict with prior assumptions \cite[see, e.g.,][]{2006:evans}.
This is most relevant when there is not enough data to overrule the prior;
as it is then unclear whether to put more trust to prior assumptions or to the observations,
posterior inferences should clearly reflect this state of uncertainty.
It was pointed out by Walter \& Augustin \cite{Walter2009a} that
both precise and imprecise models based on conjugate priors can be insensitive to prior-data conflict.

For Bayesian inference based on a single, precise conjugate prior,
learning from data amounts to averaging between prior and data
\cite[see, e.g.,][\S~1.2.3.1]{2013:diss-gw}.
%as will be illustrated in Sect.~\ref{sec:beta-binom}.
While this averaging property allows for great tractability,
it is also at the root of prior-data conflict insensitivity: 
When observed data are very much different from what is assumed in the prior,
this conflict is simply averaged out,
and is not reflected in the spread or variance of the posterior,
ultimately conveying a false sense of certainty:
A posterior with a small variance indicates that we know what's going on quite precisely -- but in case of \pdc\ we in fact do not.

Prior-data conflict is instead reflected by a larger set of posteriors,
i.e., more cautious probability statements,
when using carefully tailored sets of conjugate priors,
as suggested by \cite{Walter2009a}.
In this approach, sets of conjugate priors are defined via sets of canonical parameters,
and conditions for the shape of these parameter sets were derived that ensure \pdc\ sensitivity.
\cite{Walter2009a} then suggested a parameter set shape
that balances tractability and ease of elicitation
with desired inference properties.
The approach has since been successfully applied in common-cause failure modelling \cite{Troffaes2014a}
and system reliability estimation \cite{2015:walter}.

In the present paper, we want to further refine this approach
by complementing the increased imprecision reaction to \pdc\
with a `bonus' in precision for the case when prior and data coincide especially well,
a situation that we will call \emph{strong prior-data agreement}.
In the vein of the discussion at the end of \cite[\S~3.1.4]{2013:diss-gw},
we thus sacrifice a bit of tractability through employing a more complex parameter set shape
in order to obtain a further refined model behaviour.%
\footnote{Some first results in this direction were already presented in \cite[\S~A.2]{2013:diss-gw}.}

As such, the present work presents an idea how to define, through a novel parameter set shape,
sets of priors such that the correpsonding sets of posteriors
show both \pdc\ sensitivity and a reaction to strong prior-data agreement.
%***motivation: clever choice of prior sets
%***emphazise that this is basically an idea for defining sets of priors***
To keep things simple, we restrict ourselves here to the case of the Beta-Binomial model (see Sect.~\ref{sec:beta-binom}),
but the approach is generalisable to arbitrary canonical conjugate priors,
and we will comment on this in the concluding remarks (Sect.~\ref{sec:concluding}).

\cite{Walter2009a} used a parametrization 
that allows for an intuitive interpretation of the parameters of the Beta distribution
and clearly shows that the update step from prior to posterior
amounts to a weighted average in the conjugate setting (Sect.~\ref{sec:beta-binom}).
The new shape is instead defined in terms of another parametrization
which was recently suggested by Bickis \cite{2015:mik-isipta}.
We will therefore briefly characterise this novel parametrisation of canoncial conjugate priors (Sect.~***), %this approach relies on.
and then go on to suggest a shape in this parametrisation that accomplishes
both \pdc\ sensitivity and `bonus precision' in case of strong prior-data agreement (Sect.~***).
We propose a parametric description for such a shape
and show that it indeed leads to the desired properties (Sect.~***).
Sect.~*** concludes the paper by discussing generalizations and potential applications. 


\section{The Beta-Binomial Model in Two Parametrizations}
\label{sec:beta-binom}

Consider an experiment where there are only two possible outcomes,
\emph{success} and \emph{failure}, and success arising with some probability $p$.
The number of successes $s$ in a series of $n$ independent trials
has then a Binomial distribution with parameters $p$ and $n$,
where $n$ is known but $p \in [0,1]$ is unknown.
In short, $S\mid p \sim \bin(n,p)$, which means
\begin{align}
f(s\mid p) &= P(S = s \mid p) = {n \choose s} p^s (1-p)^{n-s},\quad s \in \{0, 1, \ldots, n\}\,.
\label{eq:binompmf}
\end{align}
In a Bayesian setting, information about unknown parameters (here, $p$) is then expressed
by a so-called prior distribution, here $f(p)$, usually derived from expert information.
In general, the posterior distribution according to Bayes' rule is hard to obtain,
especially due to the integral in the denominator of \eqref{eq:bayesrule}.
The posterior can be approximated with numerical methods,
for example the Laplace approximation, or simulation methods like MCMC (Markov chain Monte Carlo).

However, Bayesian inference not necessarily entails complex calculations and simulation methods.
With a clever choice of parametric family for the prior distribution,
the posterior distribution belongs to the same parametric family as the prior, just with updated parameters.
Such prior distributions are called \emph{conjugate} priors.
Basically, with conjugate priors one trades flexibility for tractability:
The parametric family restricts the form of the prior pdf,
but with the advantage of much easier computations.

The conjugate prior for the Binomial distribution \eqref{eq:binompmf} is the Beta distribution,
which is usually parametrized with parameters $\alpha > 0$ and $\beta > 0$,
\begin{align}
f(p\mid\alpha,\beta) &= \frac{1}{B(\alpha,\beta)}\, p^{\alpha-1}\, (1-p)^{\beta-1}\,,
\label{eq:betadensab}
\end{align}
where $B(\cdot,\cdot)$ is the Beta function.
In short, we write $p \sim \be(\alpha,\beta)$.
%$\alpha$ and $\beta$ can be interpreted as pseudocounts,
%giving the number of virtual successes and failures, respectively.
The combination of a Binomial sampling model with this conjugate Beta prior is called \emph{Beta-Binomial model},
and parameters identifying the prior distribution are often called \emph{hyperparameters}.

In the remainder of the paper, we will denote prior parameter values by an upper index~${}\uz$,
in contrast to posterior parameter values obtained after observing $n$ trials,
which are indicated by an upper index~${}\un$.
With this notational convention,
let $S\mid p \sim \bin(n,p)$ and $p \sim \be(\az,\bz)$.
Then it holds that $p \mid s \sim \be(\an,\bn)$,
where $\an$ and $\bn$ are updated, posterior parameters, obtained as
%\begin{align}
$\an = \az + s$ and $\bn= \bz + n - s$.
%\label{eq:betapostab}
%\end{align}
Note that $\az$ and $\bz$ can be interpreted as pseudocounts,
forming a hypothetical sample with $\az$ sucesses and $\bz$ failures.

We will consider now a different parametrisation of the Beta distribution
which is more useful when we generalize to sets of priors. % we find more intuitive
It is defined as
%\begin{align}
$\nz = \az + \bz$, $\yz = \frac{\az}{\az+\bz}$.
%\label{eq:betareparam}
%\end{align}
Writing $p \sim \be(\nz,\yz)$ thus corresponds to
%\begin{align}
%$f(p\mid\nz,\yz) = \frac{p^{\nz\yz-1}\, (1-p)^{\nz(1-\yz)-1}}{B(\nz\yz,\nz(1-\yz))}$.
$f(p) \propto p^{\nz\yz-1}\, (1-p)^{\nz(1-\yz)-1}$.
%\label{eq:betadensny}
%\end{align}
$\nz > 0$ and $\yz \in (0,1)$ are sometimes called \emph{canonical} parameters,
identified from rewriting the density in canonical form;
see for example \cite[pp.~202 and 272f]{2000:bernardosmith}, or \cite[\S 1.2.3.1]{2013:diss-gw}.
(This canonical form gives a common structure to all conjugacy results in exponential families.)
%In this parametrisation, the updated, posterior parameters are given by
%\begin{align}
%\nn &= \nz + n\,, &
%\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,.
%\label{eq:betapostny}
%\end{align}
%and we write $p\mid s \sim \be(\nn,\yn)$.
From the properties of the Beta distribution,
it follows that $\yz = \E[p]$ is the prior expectation for the functioning probability $p$,
and that larger $\nz$ values lead to greater concentration of probability mass around $\yz$,
since $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$.
Consequently, $\nz$ represents the prior strength
and moreover can be directly interpreted as a (total) pseudocount due to the relation $\nz = \az + \bz$.

The posterior given that $s$ out of $n$ components function,
written in terms of $\nz$ and $\yz$,
is a Beta distribution with updated parameters
\begin{align}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,.
\label{eq:nyupdate}
\end{align}
Thus, after observing $s$ successes in $n$ trials,
the posterior mean $\yn$ for $p$ is a weighted average of
the prior mean $\yz$ and $s/n$ (the observed fraction of successes),
with weights $\nz$ and $n$, respectively.
So $\nz$ takes on the same role for the prior mean $\yz$
as the sample size $n$ does for the observed mean $s/n$,
confirming its interpretation as a pseudocount.

The parametrization in terms of prior mean and prior strength (or pseudocount)
makes clear that in this conjugate setting,
learning from data corresponds to averaging between prior and data.
This form is attractive because it enhances the interpretability of the model and prior specification,
and makes Bayesian inference easily tractable.
However, it also makes clear what should be a serious concern in any Bayesian analysis:
when observed data differ greatly from what is expressed in the prior,
this conflict is simply averaged out
and is not reflected in the posterior or posterior predictive distributions.


\section{Generalized Bayesian Inference with Sets of Beta Priors}
\label{sec:setsofbetapriors}

As was shown by \cite{Walter2009a}, %Walter \& Augustin (2009),
it is possible obtain a meaningful reaction to prior-data conflict while retaining tractability
by using sets of priors $\MZ$ produced through parameter sets $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$.
%(a detailed discussion of different choices for $\PktZ$ is given in \citet[\S 3.1]{2013:diss-gw}.)
More generally, \cite[\S 3.1]{2013:diss-gw} describes a framework for
Bayesian inference using sets of conjugate priors based on arbitrary parameter sets $\PZ$.
%This sets of priors approach also allows to model vague and incomplete expert knowledge.
Applying this framework to the Beta-Binomial model described above,
each prior parameter pair $(\nz, \yz) \in \PZ$ corresponds to a Beta prior,
so one may take $\MZ$ directly as a set of Beta priors.
Alternatively, one may take the convex hull of all Beta priors with $(\nz, \yz) \in \PZ$ as $\MZ$,
i.e., $\MZ$ consists of all finite mixtures of Beta priors with $(\nz, \yz) \in \PZ$.

It is a modeling decision whether to take $\MZ$ as containing only Beta priors or also the mixtures.
In the first case, bounds for any inferences can be obtained by optimizing over $\PZ$.
In the second case, optimizing over $\PZ$ will only yield bounds for all inferences
that are \emph{linear functions} of $\nz$ and $\yz$,
as the linearity ensures that bounds must correspond to the extreme points of the convex set of priors,
which are the Beta priors with $(\nz, \yz) \in \PZ$.
For inferences not linear in $\nz$ and $\yz$, meaningful bounds might not exist.%
\footnote{Consider for example the prior variance for $p$, $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$,
being nonlinear in $\nz$ and $\yz$.
It is intuitively clear that $\V(p)$ can be larger for mixture distributions than for `pure' Beta distributions.}

In both cases, the set of posteriors $\MN$ is obtained by updating each prior in $\MZ$ according to Bayes' Rule.
This element-by-element updating can be rigorously justified
as ensuring coherence \cite[\S 2.5]{1991:walley}, and was termed ``Generalized Bayes' Rule'' by Walley \cite[\S 6.4]{1991:walley}.
In the first case, $\MN$ is a set of Beta distributions with parameters $(\nn, \yn)$,
obtained by updating $(\nz, \yz) \in \PZ$ according to \eqref{eq:nyupdate},
leading to the set of updated parameters
\begin{align}
\PN &= \Big\{ (\nn, \yn) \mid (\nz, \yz) \in \PZ = [\nzl, \nzu] \times [\yzl, \yzu] \Big\}\,.
\label{eq:paramsets}
\end{align}
In the second case, the set of Beta distributions corresponding to $(\nn, \yn) \in \PN$
forms the extreme points of the convex set of posteriors $\MN$,
such that, just like $\MZ$, $\MN$ can be described as a set of all finite mixtures of Beta distributions,
but now with $(\nn, \yn) \in \PN$, see \cite[pp.~56f]{2013:diss-gw}.

The set of posteriors $\MN$ forms the basis for all inferences,
leading to probability \emph{ranges} obtained by optimizing over $\MN$.
As a running example, in the Beta-Binomial model %as discussed in Section~\ref{sec:beta-binom},
the posterior predictive probability %$[\lpr,\upr]$
for the event that a future single draw is a success is equal to $\yn$, and so we get,
for an imprecise model $\MZ$ based on $\PZ$,
the lower and upper probability
\begin{align*}
\inf_{\PN} \yn &= \inf_{\PZ} \frac{\nz\yz + s}{\nz +n} & &\text{and} &
\sup_{\PN} \yn &= \sup_{\PZ} \frac{\nz\yz + s}{\nz +n}\,.
\end{align*}

The relation between $\PZ$ and $\MZ$, as well as between $\PN$ and $\MN$,
allows to characterize model properties through properties of $\PZ$ and $\PN$,
as is done in \cite[\S 3.1.2 -- 3.1.4]{2013:diss-gw}.
In particular, specific imprecise probability models are obtained by certain choices of $\PZ$.
The well-known Imprecise Dirichlet Model \cite{1996:walley::idm}
corresponds to a choice of $\PZ = \nz \times (\yzl, \yzu)$ where $(\yzl, \yzu) = (0,1)$.
The model proposed by \cite{2005:quaeghebeurcooman} generally assumes $\PZ = \nz \times [\yzl, \yzu]$,
and was shown to be insensitive to prior-data conflict by \cite{Walter2009a},
who proposed parameter sets $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$ instead.

%*** lower, upper $\yn$ \& range for QdC, rectangle, discuss some inference properties.
Indeed, with $\PZ = \nz \times [\yzl, \yzu]$, we have $\PN = \nn \times [\ynl, \ynu]$, where
\begin{align*}
\ynl &= \frac{\nz\yzl + s}{\nz + n} & &\text{and} &
\ynu &= \frac{\nz\yzu + s}{\nz + n}\,.
\end{align*}
The posterior imprecision in the $y$ dimension, denoted by $\Delta_y(\PN)$, is 
\begin{align*}
%\label{eq:deltay}
\Delta_y(\PN) &= \ynu - \ynl = \frac{\nz (\yzu - \yzl)}{\nz +n}\,,
\end{align*}
and so the same for any fixed $n$, independent of $s$.
In contrast, parameter sets $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$
provide prior-data conflict sensitivity, since
%\footnote{See \cite{Walter2009a} or \cite[\S 3.1.4]{2013:diss-gw} for more details.}
\begin{align*}
%\label{eq:deltay2}
\Delta_y(\PN) &= \frac{\nzu (\yzu - \yzl)}{\nzu + n} %\nonumber\\
               + \inf_{\yz \in [\yzl,\yzu]} |s/n - \yz| \frac{n (\nzu - \nzl)}{(\nzl + n)(\nzu + n)}\,.
\end{align*}

As discussed at the end of \cite[\S 3.1.4]{2013:diss-gw},
the shape of $\PZ$ thus poses a trade-off:
Less complex shapes are easy to handle and lead to tractable models,
but will offer less flexibility in expressing prior information
and may have undesired inference properties.
In contrast, more complex shades may allow for more sophisticated model behaviour
at the cost of more complex handling.


\section{A Third Parametrization for Beta Priors}
\label{sec:miksworld}

In the parametrization in terms of $\nz$ and $\yz$ as described in Section~\ref{sec:beta-binom},
a conjugate Beta prior is updated to its respective posterior by a shift in the parameter space,
given by rewriting \eqref{eq:nyupdate}:
\begin{align*}
\nz &\mapsto \nz + n\,, &
\yz &\mapsto %\frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n} = 
             \yz + \frac{s - n \yz}{\nz+n}\,.
\end{align*}
We see thus that, while the shift for the $n$ coordinate is the same for all elements $(\nz,\yz)$ of $\PZ$,
the shift in the $y$ coordinate depends on $\nz$, $s$, and the location of $\yz$ itself.
Due to this, the shape of $\PZ$ will change during the update step.
This shape change from $\PZ$ to $\PN$ is problematic for understanding of the update step in generalized Bayesian inference,
as its effects on posterior inferences are difficult to grasp.

Therefore, to isolate the influence of a set shape,
a different parametrisation of the canonical priors
in which each coordinate has the same shift in updating
would be advantageous, as then the updating of a parameter set
would correspond to a shift of the entire set within the parameter space.
Such a parametrisation has been developed by Bickis \cite{2015:mik-isipta}, %Mi\c{k}elis Bickis
and we will now briefly introduce it. %in the next subsection.

In this novel parametrisation, a prior is represented by a coordinate $(\ezz,\eoz)$,
where $\eoz$ replaces $\yz$, while $\ezz$ is just a shifted version of $\nz$.
%there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 
To keep things simple, we will not present its derivation here, this is described in detail in \cite{2015:mik-isipta}.
The relation of $(\ezz,\eoz)$ to $(\nz, \yz)$ is as follows:
%\begin{equation}
\begin{align}
\label{eq:trafotony}
%\begin{aligned}
\nz &= \ezz + 2\,, &
\yz &= \frac{\eoz}{\ezz + 2} + \frac{1}{2}\,.
%\end{aligned}
%\end{equation}
\end{align}
The domain of $\eta_0$ and $\eta_1$ in case of the Beta-Binomial model is
\begin{align}
\label{eq:eta-domain}
\Eta &= \Big\{ (\eta_0,\eta_1) \Big| \eta_0 > -2,\ |\eta_1| < \frac{1}{2}(\eta_0 + 2) \Big.\Big\}\,,
\end{align}
and the update step in terms of $\eta_0$ and $\eta_1$ is given by
\begin{equation}
\label{eq:eta-update}
\begin{aligned}
\eta_0\un &= \eta_0\uz + n\,, & 
\eta_1\un &= \eta_1\uz + \frac{1}{2}(s - (n-s)) = \eta_1\uz + s - \frac{n}{2}\,.
\end{aligned}
\end{equation}
%where $s$ is again the number of successes in the $n$ Bernoulli trials.
Each success thus
leads to a step of $1$ in the $\eta_0$ direction and of $+\frac{1}{2}$ in the $\eta_1$ direction,
while each failure
leads to a step of $1$ in the $\eta_0$ direction and of $-\frac{1}{2}$ in the $\eta_1$ direction. %
%\footnote{We will treat $s$ as a a real-valued observation in $[0,n]$
%because the continuous representation is convenient for our discussions,
%keeping in mind that in reality it can only take integer values.}




\section{Discussion and Concluding Remarks}
\label{sec:concluding}


% ------------ bibliography -------------

\printbibliography


\end{document}
