\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{biblatex}
\addbibresource{boatpaper}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{url}
\usepackage[bookmarks]{hyperref}

\setcounter{tocdepth}{3}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\pdc{prior-data conflict}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bm #1}}

\newcommand{\btheta}{\bs{\theta}}
\newcommand{\bpsi}{\bs{\psi}}
\newcommand{\bbeta}{\bs{\beta}}
\newcommand{\balpha}{\bs{\alpha}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i
\newcommand{\uinfty}{^{(\infty)}} % upper infty

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\def\yz{y\uz}
\def\yn{y\un}
\def\yi{y\ui}

\def\yzl{\ul{y}\uz}
\def\yzu{\ol{y}\uz}

\def\ynl{\ul{y}\un}
\def\ynu{\ol{y}\un}

\def\yil{\ul{y}\ui}
\def\yiu{\ol{y}\ui}

\def\nz{n\uz}
\def\nn{n\un}
\def\ni{n\ui}

\def\nzl{\ul{n}\uz}
\def\nzu{\ol{n}\uz}

\def\nnl{\ul{n}\un}
\def\nnu{\ol{n}\un}

\def\nil{\ul{n}\ui}
\def\niu{\ol{n}\ui}

\def\taux{\tau(x)}
\def\ttau{\tilde{\tau}}
\def\ttaux{\ttau(x)}

\def\pz{\psi\uz}
\def\pn{\psi\un}

\def\Y{\mathcal{Y}}
\def\YZ{\Y\uz}
\def\YN{\Y\un}
\def\YI{\Y\ui}
\def\N{\mathcal{N}}
\def\NZ{\N\uz}
\def\NN{\N\un}
\def\PZ{\text{I}\!\Pi\uz}
%\def\PZero{\PZ}
\def\PN{\text{I}\!\Pi\un}
\def\Pinfty{\text{I}\!\Pi\uinfty}
\def\MZ{\mathcal{M}\uz}
\def\MN{\mathcal{M}\un}

\newcommand{\az}{\alpha\uz}
\newcommand{\an}{\alpha\un}
\newcommand{\bz}{\beta\uz}
\newcommand{\bn}{\beta\un}

\def\Eta{\mathrm{H}}
\def\EZ{\mathrm{H}\uz}
\def\EN{\mathrm{H}\un}

\def\ezl{\ul{\eta}_0}
\def\ezu{\ol{\eta}_0}

\def\ezz{\eta_0\uz}
\def\ezn{\eta_0\un}
\def\eoz{\eta_1\uz}
\def\eon{\eta_1\un}

\def\ezzl{\ul{\eta}_0\uz}
\def\ezzu{\ol{\eta}_0\uz}
\def\eznl{\ul{\eta}_0\un}
\def\ezzn{\ol{\eta}_0\un}

\def\eozl{\ul{\eta}_1\uz}
\def\eozu{\ol{\eta}_1\uz}
\def\eonl{\ul{\eta}_1\un}
\def\eonu{\ol{\eta}_1\un}

\def\eol{\ul{\eta}_1}
\def\eou{\ol{\eta}_1}

\def\czl{\ul{c}\uz}
\def\czu{\ol{c}\uz}

\newcommand{\cdf}{\operatorname{F}}
\newcommand{\p}{\operatorname{P}}
\newcommand{\q}{\operatorname{Q}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\med}{\operatorname{med}} % Median
\newcommand{\modus}{\operatorname{mode}} % Mode
\newcommand{\logit}{\operatorname{logit}} % logit

%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}

\def\El{\ul{\E}}
\def\Eu{\ol{\E}}

\def\Pl{\ul{\p}}
\def\Pu{\ol{\p}}

\newcommand{\ber}{\operatorname{Ber}}   % Bernoulli Distribution
\newcommand{\bin}{\operatorname{Binom}} % Binomial Distribution
\newcommand{\be}{\operatorname{Beta}}   % Beta Distribution
\newcommand{\B}{\operatorname{B}}   % Beta Function



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Manuscript body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Sets of Prior Distributions\\ for Reflecting Prior-Data Conflict\\ and Strong Prior-Data Agreement}

% a short form should be given in case it is too long for the running head
\titlerunning{Sets of Priors for Reflecting Prior-Data Conflict and Agreement}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Gero Walter\inst{1}%
\thanks{Gero Walter was supported by the Dinalog project
``Coordinated Advanced Maintenance and Logistics Planning for the Process Industries'' (CAMPI).}%
\and Frank P.A.\ Coolen\inst{2}}
%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{%
School of Industrial Engineering,\\
Eindhoven University of Technology, Eindhoven, NL\\
\url{g.m.walter@tue.nl}
\and
Department of Mathematical Sciences,\\
Durham University, Durham, UK\\
\url{frank.coolen@durham.ac.uk}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\maketitle


\begin{abstract}
Bayesian inference allows to combine observations with (possibly subjective) prior knowledge in the reasoning process.
The choice of a particular prior distribution to represent the available prior knowledge is, however,
often debatable, especially when prior knowledge is limited or data are scarce,
as then posterior inferences are higly dependent on the choice of prior.
Robust Bayesian analysis accounts for this issue by
inquiring whether posterior inferences change substantially
when the prior distribution is varied within a set of distributions that contains all `reasonable' priors.
Similar, but slightly different in scope, is the imprecise probability approach,
formalising the idea that sets of probability distributions
%(or, equivalently, interval-valued previsions or sets of desirable gambles)
should be taken to model prior knowledge more acurrately.
With imprecise probability, modeling prior-data conflict sensitivity is possible:
Models have been proposed where the ranges of posterior inferences
are substantially larger when prior and data are in conflict.
Here we propose a new method for generating parameter prior sets in a conjugate setting
that, in addition to prior-data conflict sensitivity, allow to mirror \emph{strong prior-data agreement},
i.e., the case when prior and data coincide especially well, by increased posterior precision.
Although presented here for the case of binary data only,
it is easily extensible to the general exponential family case.
\keywords{Bayesian inference, strong prior-data agreement, prior-data conflict, imprecise probability, conjugate priors}
\end{abstract}


\section{Introduction}

The Bayesian approach to inference \cite[see, e.g.,][]{2007:robert,2005:ruggeri} 
offers the advantage to combine data and prior knowledge in a unified reasoning process.
The prior knowledge, i.e., information extraneous to data, is usually provided in the form of expert knowledge.

Bayesian inference usually starts with defining a parametric \emph{sample model},
denoted by $f(\vec{x} \mid \vartheta)$,
a conditional distribution of data $\vec{x} = (x_1, \ldots, x_n)$ given the value for a parameter $\vartheta$.
Prior information is then encoded by a so-called \emph{prior distribution} $p(\vartheta)$,
expressing the expert's opinion on probable values of $\vartheta$.
In light of the sample $\vec{x}$, the prior distribution is updated by Bayes' Rule
to obtain the so-called \emph{posterior distribution}
\begin{align}
\label{eq:bayesrule}
p(\vartheta\mid\vec{x}) &= \frac{f(\vec{x}\mid\vartheta) f(\vartheta)}{\int f(\vec{x}\mid\vartheta) f(\vartheta) \dd \vartheta}
                         \propto f(\vec{x}\mid\vartheta) \cdot p(\vartheta)\,.
\end{align}
The posterior distribution is understood to comprise all information from the sample and the prior knowledge.
It therefore underlies all further inferences on the parameter $\vartheta$,
like point estimators, interval estimators,
or the \emph{posterior predictive distribution},
giving the distribution of further observations based on the posterior.

However, the choice for the prior distribution to encode the given expert knowledge is often debatable,
and a specific choice of prior is difficult to justify.
A way to deal with this is to employ sensitivity analysis,
i.e., studying the effect of different choices of prior distribution on the quantities of interest.
This idea has been explored in systematic sensitivity analysis, or robust Bayesian methods;
for an overview on this approach, see, e.g.,
\cite{1994:berger} or \cite{2000:rios}. %\citeNP{2005:ruggeri}, \citeNP{2000:bergerinsuaruggeri}

The work we present here can be seen as belonging to the robust Bayesian approach, as our work uses sets of priors.
However, our focus and interpretation is slightly different,
as we consider the result of our procedure, sets of posterior distributions, as the proper result,
while a robust Bayesian would base his analyses on a single posterior from the set
in case (s)he was able to conclude that quantities of interest are not `too sensitive' to the choice of prior.
In contrast, our viewpoint is rooted in the theory of imprecise or interval probability \cite{itip,1991:walley},
where sets of distributions are used to express the precision of probability statements themselves:
the smaller the set of posteriors, the more precise the probability statements.

The central idea in imprecise probability is that the magnitude of a set of distributions,
and, in turn, the width of probability ranges based on it,
should reflect the precision of probabilistic knowledge.
A situation where this relation should especially hold is that of \emph{\pdc}:
% from abstract esrelpaper
%A problem that can arise in Bayesian inference is called prior-data conflict:
From the viewpoint of the prior $p(\vartheta)$, the observed data $\vec{x}$ seem very surprising,
i.e., information from data is in conflict with prior assumptions \cite[see, e.g.,][]{2006:evans}.
This is most relevant when there is not enough data to overrule the prior;
as it is then unclear whether to put more trust to prior assumptions or to the observations,
posterior inferences should clearly reflect this state of uncertainty.
It was pointed out by Walter \& Augustin \cite{Walter2009a} that
both precise and imprecise models based on conjugate priors can be insensitive to prior-data conflict.

For Bayesian inference based on a single, precise conjugate prior,
learning from data amounts to averaging between prior and data
\cite[see, e.g.,][\S~1.2.3.1]{2013:diss-gw}.
%as will be illustrated in Sect.~\ref{sec:beta-binom}.
While this averaging property allows for great tractability,
it is also at the root of prior-data conflict insensitivity: 
When observed data are very much different from what is assumed in the prior,
this conflict is simply averaged out,
and is not reflected in the spread or variance of the posterior,
ultimately conveying a false sense of certainty:
A posterior with a small variance indicates that we know what's going on quite precisely -- but in case of \pdc\ we in fact do not.

Prior-data conflict is instead reflected by a larger set of posteriors,
i.e., more cautious probability statements,
when using carefully tailored sets of conjugate priors,
as suggested by \cite{Walter2009a}.
In this approach, sets of conjugate priors are defined via sets of canonical parameters,
and conditions for the shape of these parameter sets were derived that ensure \pdc\ sensitivity.
\cite{Walter2009a} then suggested a parameter set shape
that balances tractability and ease of elicitation
with desired inference properties.
The approach has since been successfully applied in common-cause failure modelling \cite{Troffaes2014a}
and system reliability estimation \cite{2015:walter}.

In the present paper, we want to further refine this approach
by complementing the increased imprecision reaction to \pdc\
with a `bonus' in precision for the case when prior and data coincide especially well,
a situation that we will call \emph{strong prior-data agreement}.
In the vein of the discussion at the end of \cite[\S~3.1.4]{2013:diss-gw},
we thus sacrifice a bit of tractability through employing a more complex parameter set shape
in order to obtain a further refined model behaviour.%
\footnote{Some first results in this direction were already presented in \cite[\S~A.2]{2013:diss-gw}.}

As such, the present work presents an idea how to define, through a novel parameter set shape,
sets of priors such that the correpsonding sets of posteriors
show both \pdc\ sensitivity and a reaction to strong prior-data agreement.
%***motivation: clever choice of prior sets
%***emphazise that this is basically an idea for defining sets of priors***
To keep things simple, we restrict ourselves here to the case of the Beta-Binomial model (see Sect.~\ref{sec:beta-binom}),
but the approach is generalisable to arbitrary canonical conjugate priors,
and we will comment on this in the concluding remarks (Sect.~\ref{sec:concluding}).

\cite{Walter2009a} used a parametrization 
that allows for an intuitive interpretation of the parameters of the Beta distribution
and clearly shows that the update step from prior to posterior
amounts to a weighted average in the conjugate setting (Sect.~\ref{sec:beta-binom}).
The new shape is instead defined in terms of another parametrization
which was recently suggested by Bickis \cite{2015:mik-isipta}.
We will therefore briefly characterise this novel parametrisation of canoncial conjugate priors (Sect.~***), %this approach relies on.
and then go on to suggest a shape in this parametrisation that accomplishes
both \pdc\ sensitivity and `bonus precision' in case of strong prior-data agreement (Sect.~***).
We propose a parametric description for such a shape
and show that it indeed leads to the desired properties (Sect.~***).
Sect.~*** concludes the paper by discussing generalizations and potential applications. 


\section{The Beta-Binomial Model}
\label{sec:beta-binom}

Consider an experiment where there are only two possible outcomes,
\emph{success} and \emph{failure}, and success arising with some probability $p$.
The number of successes $s$ in a series of $n$ independent trials
has then a Binomial distribution with parameters $p$ and $n$,
where $n$ is known but $p \in [0,1]$ is unknown.
In short, $S\mid p \sim \bin(n,p)$, which means
\begin{align}
f(s\mid p) &= P(S = s \mid p) = {n \choose s} p^s (1-p)^{n-s},\quad s \in \{0, 1, \ldots, n\}\,.
\label{eq:binompmf}
\end{align}
In a Bayesian setting, information about unknown parameters (here, $p$) is then expressed
by a so-called prior distribution, here $f(p)$, usually derived from expert information.
In general, the posterior distribution according to Bayes' rule is hard to obtain,
especially due to the integral in the denominator of \eqref{eq:bayesrule}.
The posterior can be approximated with numerical methods,
for example the Laplace approximation, or simulation methods like MCMC (Markov chain Monte Carlo).

However, Bayesian inference not necessarily entails complex calculations and simulation methods.
With a clever choice of parametric family for the prior distribution,
the posterior distribution belongs to the same parametric family as the prior, just with updated parameters.
Such prior distributions are called \emph{conjugate} priors.
Basically, with conjugate priors one trades flexibility for tractability:
The parametric family restricts the form of the prior pdf,
but with the advantage of much easier computations.

The conjugate prior for the Binomial distribution \eqref{eq:binompmf} is the Beta distribution,
which is usually parametrised with parameters $\alpha > 0$ and $\beta > 0$,
\begin{align}
f(p\mid\alpha,\beta) &= \frac{1}{B(\alpha,\beta)}\, p^{\alpha-1}\, (1-p)^{\beta-1}\,,
\label{eq:betadensab}
\end{align}
where $B(\cdot,\cdot)$ is the Beta function.
In short, we write $p \sim \be(\alpha,\beta)$.

In the rest of the paper, we will denote prior parameter values by an upper index~${}\uz$,
and updated, posterior parameter values by an upper index~${}\un$.
With this notational convention,
let $S\mid p \sim \bin(n,p)$ and $p \sim \be(\az,\bz)$.
Then it holds that $p \mid s \sim \be(\an,\bn)$,
where $\an$ and $\bn$ are updated, posterior parameters, obtained as
%\begin{align}
$\an = \az + s$ and $\bn= \bz + n - s$.
%\label{eq:betapostab}
%\end{align}
Note that $\az$ and $\bz$ can be interpreted as pseudocounts,
forming a hypothetical sample with $\az$ sucesses and $\bz$ failures.





\section{Discussion and Concluding Remarks}
\label{sec:concluding}


% ------------ bibliography -------------

\printbibliography


\end{document}
