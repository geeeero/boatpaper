\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{biblatex}
\addbibresource{boatpaper}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{url}
\usepackage[bookmarks]{hyperref}

\setcounter{tocdepth}{3}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\pdc{prior-data conflict}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bm #1}}

\newcommand{\btheta}{\bs{\theta}}
\newcommand{\bpsi}{\bs{\psi}}
\newcommand{\bbeta}{\bs{\beta}}
\newcommand{\balpha}{\bs{\alpha}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i
\newcommand{\uinfty}{^{(\infty)}} % upper infty

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\def\yz{y\uz}
\def\yn{y\un}
\def\yi{y\ui}

\def\yzl{\ul{y}\uz}
\def\yzu{\ol{y}\uz}

\def\ynl{\ul{y}\un}
\def\ynu{\ol{y}\un}

\def\yil{\ul{y}\ui}
\def\yiu{\ol{y}\ui}

\def\nz{n\uz}
\def\nn{n\un}
\def\ni{n\ui}

\def\nzl{\ul{n}\uz}
\def\nzu{\ol{n}\uz}

\def\nnl{\ul{n}\un}
\def\nnu{\ol{n}\un}

\def\nil{\ul{n}\ui}
\def\niu{\ol{n}\ui}

\def\taux{\tau(x)}
\def\ttau{\tilde{\tau}}
\def\ttaux{\ttau(x)}

\def\pz{\psi\uz}
\def\pn{\psi\un}

\def\Y{\mathcal{Y}}
\def\YZ{\Y\uz}
\def\YN{\Y\un}
\def\YI{\Y\ui}
\def\N{\mathcal{N}}
\def\NZ{\N\uz}
\def\NN{\N\un}
%\def\PZ{\text{I}\!\Pi\uz}
\def\PZ{I\!\!\Pi\uz}
%\def\PN{\text{I}\!\Pi\un}
\def\PN{I\!\!\Pi\un}
%\def\Pinfty{\text{I}\!\Pi\uinfty}
\def\MZ{\mathcal{M}\uz}
\def\MN{\mathcal{M}\un}

\newcommand{\az}{\alpha\uz}
\newcommand{\an}{\alpha\un}
\newcommand{\bz}{\beta\uz}
\newcommand{\bn}{\beta\un}

\def\Eta{\mathrm{H}}
\def\EZ{\mathrm{H}\uz}
\def\EN{\mathrm{H}\un}

\newcommand{\ez}{\eta_0}
\newcommand{\eo}{\eta_1}

\def\ezl{\ul{\eta}_0}
\def\ezu{\ol{\eta}_0}

\def\ezz{\eta_0\uz}
\def\ezn{\eta_0\un}
\def\eoz{\eta_1\uz}
\def\eon{\eta_1\un}

\def\ezzl{\ul{\eta}_0\uz}
\def\ezzu{\ol{\eta}_0\uz}
\def\eznl{\ul{\eta}_0\un}
\def\ezzn{\ol{\eta}_0\un}

\def\eozl{\ul{\eta}_1\uz}
\def\eozu{\ol{\eta}_1\uz}
\def\eonl{\ul{\eta}_1\un}
\def\eonu{\ol{\eta}_1\un}

\def\eol{\ul{\eta}_1}
\def\eou{\ol{\eta}_1}

\def\czl{\ul{c}\uz}
\def\czu{\ol{c}\uz}

\def\cnl{\ul{c}\un}
\def\cnu{\ol{c}\un}

\newcommand{\cdf}{\operatorname{F}}
\newcommand{\p}{\operatorname{P}}
\newcommand{\q}{\operatorname{Q}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\med}{\operatorname{med}} % Median
\newcommand{\modus}{\operatorname{mode}} % Mode
\newcommand{\logit}{\operatorname{logit}} % logit

%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}

\def\El{\ul{\E}}
\def\Eu{\ol{\E}}

\def\Pl{\ul{\p}}
\def\Pu{\ol{\p}}

\newcommand{\ber}{\operatorname{Ber}}   % Bernoulli Distribution
\newcommand{\bin}{\operatorname{Binom}} % Binomial Distribution
\newcommand{\be}{\operatorname{Beta}}   % Beta Distribution
\newcommand{\B}{\operatorname{B}}   % Beta Function



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Manuscript body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Sets of Prior Distributions\\ for Reflecting Prior-Data Conflict\\ and Strong Prior-Data Agreement}

% a short form should be given in case it is too long for the running head
\titlerunning{Sets of Priors for Reflecting Prior-Data Conflict and Agreement}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Gero Walter\inst{1}%
\thanks{Gero Walter was supported by the Dinalog project
``Coordinated Advanced Maintenance and Logistics Planning for the Process Industries'' (CAMPI).}%
\and Frank P.A.\ Coolen\inst{2}}
%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{%
School of Industrial Engineering,\\
Eindhoven University of Technology, Eindhoven, NL\\
\url{g.m.walter@tue.nl}
\and
Department of Mathematical Sciences,\\
Durham University, Durham, UK\\
\url{frank.coolen@durham.ac.uk}
}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\maketitle


\begin{abstract}
Bayesian inference allows to combine observations with (possibly subjective) prior knowledge in the reasoning process.
The choice of a particular prior distribution to represent the available prior knowledge is, however,
often debatable, especially when prior knowledge is limited or data are scarce,
as then posterior inferences are higly dependent on the choice of prior.
Robust Bayesian analysis accounts for this issue by
inquiring whether posterior inferences change substantially
when the prior distribution is varied within a set of distributions that contains all `reasonable' priors.
Similar, but slightly different in scope, is the imprecise probability approach,
formalising the idea that sets of probability distributions
%(or, equivalently, interval-valued previsions or sets of desirable gambles)
should be taken to model prior knowledge more acurrately.
With imprecise probability, modeling prior-data conflict sensitivity is possible:
Models have been proposed where the ranges of posterior inferences
are substantially larger when prior and data are in conflict.
Here we propose a new method for generating parameter prior sets in a conjugate setting
that, in addition to prior-data conflict sensitivity, allow to mirror \emph{strong prior-data agreement},
i.e., the case when prior and data coincide especially well, by increased posterior precision.
Although presented here for the case of binary data only,
it is easily extensible to the general exponential family case.
\keywords{Bayesian inference, strong prior-data agreement, prior-data conflict, imprecise probability, conjugate priors}
\end{abstract}


\section{Introduction}

The Bayesian approach to inference \cite[see, e.g.,][]{2007:robert,2005:ruggeri} 
offers the advantage to combine data and prior knowledge in a unified reasoning process.
The prior knowledge, i.e., information extraneous to data, is usually provided in the form of expert knowledge.

Bayesian inference usually starts with defining a parametric \emph{sample model},
denoted by $f(\vec{x} \mid \vartheta)$,
a conditional distribution of data $\vec{x} = (x_1, \ldots, x_n)$ given the value for a parameter $\vartheta$.
Prior information is then encoded by a so-called \emph{prior distribution} $p(\vartheta)$,
expressing the expert's opinion on probable values of $\vartheta$.
In light of the sample $\vec{x}$, the prior distribution is updated by Bayes' Rule
to obtain the so-called \emph{posterior distribution}
\begin{align}
\label{eq:bayesrule}
p(\vartheta\mid\vec{x}) &= \frac{f(\vec{x}\mid\vartheta) f(\vartheta)}{\int f(\vec{x}\mid\vartheta) f(\vartheta) \dd \vartheta}
                         \propto f(\vec{x}\mid\vartheta) \cdot p(\vartheta)\,.
\end{align}
The posterior distribution is understood to comprise all information from the sample and the prior knowledge.
It therefore underlies all further inferences on the parameter $\vartheta$,
like point estimators, interval estimators,
or the \emph{posterior predictive distribution},
giving the distribution of further observations based on the posterior.

However, the choice for the prior distribution to encode the given expert knowledge is often debatable,
and a specific choice of prior is difficult to justify.
A way to deal with this is to employ sensitivity analysis,
i.e., studying the effect of different choices of prior distribution on the quantities of interest.
This idea has been explored in systematic sensitivity analysis, or robust Bayesian methods;
for an overview on this approach, see, e.g.,
\cite{1994:berger} or \cite{2000:rios}. %\citeNP{2005:ruggeri}, \citeNP{2000:bergerinsuaruggeri}

The work we present here can be seen as belonging to the robust Bayesian approach, as our work uses sets of priors.
However, our focus and interpretation is slightly different,
as we consider the result of our procedure, sets of posterior distributions, as the proper result,
while a robust Bayesian would base his analyses on a single posterior from the set
in case (s)he was able to conclude that quantities of interest are not `too sensitive' to the choice of prior.
In contrast, our viewpoint is rooted in the theory of imprecise or interval probability \cite{itip,1991:walley},
where sets of distributions are used to express the precision of probability statements themselves:
the smaller the set of posteriors, the more precise the probability statements.

A central idea in imprecise probability is that the magnitude of a set of distributions,
and, in turn, the width of probability ranges based on it,
should reflect the precision of probabilistic knowledge.
A situation where this relation should especially hold is that of \emph{\pdc}:
% from abstract esrelpaper
%A problem that can arise in Bayesian inference is called prior-data conflict:
From the viewpoint of the prior $p(\vartheta)$, the observed data $\vec{x}$ seem very surprising,
i.e., information from data is in conflict with prior assumptions \cite[see, e.g.,][]{2006:evans}.
This is most relevant when there is not enough data to overrule the prior;
as it is then unclear whether to put more trust to prior assumptions or to the observations,
posterior inferences should clearly reflect this state of uncertainty.
It was pointed out by Walter \& Augustin \cite{Walter2009a} that
both precise and imprecise models based on conjugate priors can be insensitive to prior-data conflict.

For Bayesian inference based on a single, precise conjugate prior,
learning from data amounts to averaging between prior and data
\cite[see, e.g.,][\S~1.2.3.1]{2013:diss-gw}.
%as will be illustrated in Sect.~\ref{sec:beta-binom}.
While this averaging property allows for great tractability,
it is also at the root of prior-data conflict insensitivity: 
When observed data are very much different from what is assumed in the prior,
this conflict is simply averaged out,
and is not reflected in the spread or variance of the posterior,
ultimately conveying a false sense of certainty:
A posterior with a small variance indicates that we know what's going on quite precisely -- but in case of \pdc\ we in fact do not.

Prior-data conflict is instead reflected by a larger set of posteriors,
i.e., more cautious probability statements,
when using carefully tailored sets of conjugate priors,
as suggested by \cite{Walter2009a}.
In this approach, sets of conjugate priors are defined via sets of canonical parameters,
and conditions for the shape of these parameter sets were derived that ensure \pdc\ sensitivity.
\cite{Walter2009a} then suggested a parameter set shape
that balances tractability and ease of elicitation
with desired inference properties.
The approach has since been successfully applied in common-cause failure modelling \cite{Troffaes2014a}
and system reliability estimation \cite{2015:walter}.

In the present paper, we want to further refine this approach
by complementing the increased imprecision reaction to \pdc\
with a `bonus' in precision for the case when prior and data coincide especially well,
a situation that we will call \emph{strong prior-data agreement}.
In the vein of the discussion at the end of \cite[\S~3.1.4]{2013:diss-gw},
we thus sacrifice a bit of tractability through employing a more complex parameter set shape
in order to obtain a further refined model behaviour.%
\footnote{Some first results in this direction were already presented in \cite[\S~A.2]{2013:diss-gw}.}

As such, the present work presents an idea how to define, through a novel parameter set shape,
sets of priors such that the correpsonding sets of posteriors
show both \pdc\ sensitivity and a reaction to strong prior-data agreement.
%***motivation: clever choice of prior sets
%***emphazise that this is basically an idea for defining sets of priors***
To keep things simple, we restrict ourselves here to the case of the Beta-Binomial model (see Sect.~\ref{sec:beta-binom}),
but the approach is generalisable to arbitrary canonical conjugate priors,
and we will comment on this in the concluding remarks (Sect.~\ref{sec:concluding}).

\cite{Walter2009a} used a parametrization 
that allows for an intuitive interpretation of the parameters of the Beta distribution
and clearly shows that the update step from prior to posterior
amounts to a weighted average in the conjugate setting (Sect.~\ref{sec:beta-binom}).
The new shape is instead defined in terms of another parametrization
which was recently suggested by Bickis \cite{2015:mik-isipta}.
We will therefore briefly characterise this novel parametrisation of canoncial conjugate priors (Sect.~***), %this approach relies on.
and then go on to suggest a shape in this parametrisation that accomplishes
both \pdc\ sensitivity and `bonus precision' in case of strong prior-data agreement (Sect.~***).
We propose a parametric description for such a shape
and show that it indeed leads to the desired properties (Sect.~***).
Sect.~*** concludes the paper by discussing generalizations and potential applications. 


\section{The Beta-Binomial Model in Two Parametrizations}
\label{sec:beta-binom}

Consider an experiment where there are only two possible outcomes,
\emph{success} and \emph{failure}, and success arising with some probability $p$.
The number of successes $s$ in a series of $n$ independent trials
has then a Binomial distribution with parameters $p$ and $n$,
where $n$ is known but $p \in [0,1]$ is unknown.
In short, $S\mid p \sim \bin(n,p)$, which means
\begin{align}
f(s\mid p) &= P(S = s \mid p) = {n \choose s} p^s (1-p)^{n-s},\quad s \in \{0, 1, \ldots, n\}\,.
\label{eq:binompmf}
\end{align}
In a Bayesian setting, information about unknown parameters (here, $p$) is then expressed
by a so-called prior distribution, here $f(p)$, usually derived from expert information.
In general, the posterior distribution according to Bayes' rule is hard to obtain,
especially due to the integral in the denominator of \eqref{eq:bayesrule}.
The posterior can be approximated with numerical methods,
for example the Laplace approximation, or simulation methods like MCMC (Markov chain Monte Carlo).

However, Bayesian inference not necessarily entails complex calculations and simulation methods.
With a clever choice of parametric family for the prior distribution,
the posterior distribution belongs to the same parametric family as the prior, just with updated parameters.
Such prior distributions are called \emph{conjugate} priors.
Basically, with conjugate priors one trades flexibility for tractability:
The parametric family restricts the form of the prior pdf,
but with the advantage of much easier computations.

The conjugate prior for the Binomial distribution \eqref{eq:binompmf} is the Beta distribution,
which is usually parametrized with parameters $\alpha > 0$ and $\beta > 0$,
\begin{align}
f(p\mid\alpha,\beta) &= \frac{1}{B(\alpha,\beta)}\, p^{\alpha-1}\, (1-p)^{\beta-1}\,,
\label{eq:betadensab}
\end{align}
where $B(\cdot,\cdot)$ is the Beta function.
In short, we write $p \sim \be(\alpha,\beta)$.
%$\alpha$ and $\beta$ can be interpreted as pseudocounts,
%giving the number of virtual successes and failures, respectively.
The combination of a Binomial sampling model with this conjugate Beta prior is called \emph{Beta-Binomial model},
and parameters identifying the prior distribution are often called \emph{hyperparameters}.

In the remainder of the paper, we will denote prior parameter values by an upper index~${}\uz$,
in contrast to posterior parameter values obtained after observing $n$ trials,
which are indicated by an upper index~${}\un$.
With this notational convention,
let $S\mid p \sim \bin(n,p)$ and $p \sim \be(\az,\bz)$.
Then it holds that $p \mid s \sim \be(\an,\bn)$,
where $\an$ and $\bn$ are updated, posterior parameters, obtained as
%\begin{align}
$\an = \az + s$ and $\bn= \bz + n - s$.
%\label{eq:betapostab}
%\end{align}
Note that $\az$ and $\bz$ can be interpreted as pseudocounts,
forming a hypothetical sample with $\az$ sucesses and $\bz$ failures.

We will consider now a different parametrisation of the Beta distribution
which is more useful when we generalize to sets of priors. % we find more intuitive
It is defined as
%\begin{align}
$\nz = \az + \bz$, $\yz = \frac{\az}{\az+\bz}$.
%\label{eq:betareparam}
%\end{align}
Writing $p \sim \be(\nz,\yz)$ thus corresponds to
%\begin{align}
%$f(p\mid\nz,\yz) = \frac{p^{\nz\yz-1}\, (1-p)^{\nz(1-\yz)-1}}{B(\nz\yz,\nz(1-\yz))}$.
$f(p) \propto p^{\nz\yz-1}\, (1-p)^{\nz(1-\yz)-1}$.
%\label{eq:betadensny}
%\end{align}
$\nz > 0$ and $\yz \in (0,1)$ are sometimes called \emph{canonical} parameters,
identified from rewriting the density in canonical form;
see for example \cite[pp.~202 and 272f]{2000:bernardosmith}, or \cite[\S 1.2.3.1]{2013:diss-gw}.
(This canonical form gives a common structure to all conjugacy results in exponential families.)
%In this parametrisation, the updated, posterior parameters are given by
%\begin{align}
%\nn &= \nz + n\,, &
%\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,.
%\label{eq:betapostny}
%\end{align}
%and we write $p\mid s \sim \be(\nn,\yn)$.
From the properties of the Beta distribution,
it follows that $\yz = \E[p]$ is the prior expectation for the functioning probability $p$,
and that larger $\nz$ values lead to greater concentration of probability mass around $\yz$,
since $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$.
Consequently, $\nz$ represents the prior strength
and moreover can be directly interpreted as a (total) pseudocount due to the relation $\nz = \az + \bz$.

The posterior given that $s$ out of $n$ components function,
written in terms of $\nz$ and $\yz$,
is a Beta distribution with updated parameters
\begin{align}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,.
\label{eq:nyupdate}
\end{align}
Thus, after observing $s$ successes in $n$ trials,
the posterior mean $\yn$ for $p$ is a weighted average of
the prior mean $\yz$ and $s/n$ (the observed fraction of successes),
with weights $\nz$ and $n$, respectively.
So $\nz$ takes on the same role for the prior mean $\yz$
as the sample size $n$ does for the observed mean $s/n$,
confirming its interpretation as a pseudocount.

The parametrization in terms of prior mean and prior strength (or pseudocount)
makes clear that in this conjugate setting,
learning from data corresponds to averaging between prior and data.
This form is attractive because it enhances the interpretability of the model and prior specification,
and makes Bayesian inference easily tractable.
However, it also makes clear what should be a serious concern in any Bayesian analysis:
when observed data differ greatly from what is expressed in the prior,
this conflict is simply averaged out
and is not reflected in the posterior or posterior predictive distributions.


\section{Generalized Bayesian Inference with Sets of Beta Priors}
\label{sec:setsofbetapriors}

As was shown by \cite{Walter2009a}, %Walter \& Augustin (2009),
it is possible obtain a meaningful reaction to prior-data conflict while retaining tractability
by using sets of priors $\MZ$ produced through parameter sets $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$.
%(a detailed discussion of different choices for $\PktZ$ is given in \citet[\S 3.1]{2013:diss-gw}.)
More generally, \cite[\S 3.1]{2013:diss-gw} describes a framework for
Bayesian inference using sets of conjugate priors based on arbitrary parameter sets $\PZ$.
%This sets of priors approach also allows to model vague and incomplete expert knowledge.
Applying this framework to the Beta-Binomial model described above,
each prior parameter pair $(\nz, \yz) \in \PZ$ corresponds to a Beta prior,
so one may take $\MZ$ directly as a set of Beta priors.
Alternatively, one may take the convex hull of all Beta priors with $(\nz, \yz) \in \PZ$ as $\MZ$,
i.e., $\MZ$ consists of all finite mixtures of Beta priors with $(\nz, \yz) \in \PZ$.

It is a modeling decision whether to take $\MZ$ as containing only Beta priors or also the mixtures.
In the first case, bounds for any inferences can be obtained by optimizing over $\PZ$.
In the second case, optimizing over $\PZ$ will only yield bounds for all inferences
that are \emph{linear functions} of $\nz$ and $\yz$,
as the linearity ensures that bounds must correspond to the extreme points of the convex set of priors,
which are the Beta priors with $(\nz, \yz) \in \PZ$.
For inferences not linear in $\nz$ and $\yz$, meaningful bounds might not exist.%
\footnote{Consider for example the prior variance for $p$, $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$,
being nonlinear in $\nz$ and $\yz$.
It is intuitively clear that $\V(p)$ can be larger for mixture distributions than for `pure' Beta distributions.}

In both cases, the set of posteriors $\MN$ is obtained by updating each prior in $\MZ$ according to Bayes' Rule.
This element-by-element updating can be rigorously justified
as ensuring coherence \cite[\S 2.5]{1991:walley}, and was termed ``Generalized Bayes' Rule'' by Walley \cite[\S 6.4]{1991:walley}.
In the first case, $\MN$ is a set of Beta distributions with parameters $(\nn, \yn)$,
obtained by updating $(\nz, \yz) \in \PZ$ according to \eqref{eq:nyupdate},
leading to the set of updated parameters
\begin{align}
\PN &= \Big\{ (\nn, \yn) \mid (\nz, \yz) \in \PZ = [\nzl, \nzu] \times [\yzl, \yzu] \Big\}\,.
\label{eq:paramsets}
\end{align}
In the second case, the set of Beta distributions corresponding to $(\nn, \yn) \in \PN$
forms the extreme points of the convex set of posteriors $\MN$,
such that, just like $\MZ$, $\MN$ can be described as a set of all finite mixtures of Beta distributions,
but now with $(\nn, \yn) \in \PN$, see \cite[pp.~56f]{2013:diss-gw}.

The set of posteriors $\MN$ forms the basis for all inferences,
leading to probability \emph{ranges} obtained by optimizing over $\MN$.
As a running example, in the Beta-Binomial model %as discussed in Section~\ref{sec:beta-binom},
the posterior predictive probability %$[\lpr,\upr]$
for the event that a future single draw is a success is equal to $\yn$, and so we get,
for an imprecise model $\MZ$ based on $\PZ$,
the lower and upper probability
\begin{align*}
\inf_{\PN} \yn &= \inf_{\PZ} \frac{\nz\yz + s}{\nz +n} & &\text{and} &
\sup_{\PN} \yn &= \sup_{\PZ} \frac{\nz\yz + s}{\nz +n}\,.
\end{align*}

The relation between $\PZ$ and $\MZ$, as well as between $\PN$ and $\MN$,
allows to characterize model properties through properties of $\PZ$ and $\PN$,
as is done in \cite[\S 3.1.2 -- 3.1.4]{2013:diss-gw}.
In particular, specific imprecise probability models are obtained by certain choices of $\PZ$.
The well-known Imprecise Dirichlet Model \cite{1996:walley::idm}
corresponds to a choice of $\PZ = \nz \times (\yzl, \yzu)$ where $(\yzl, \yzu) = (0,1)$.
The model proposed by \cite{2005:quaeghebeurcooman} generally assumes $\PZ = \nz \times [\yzl, \yzu]$,
and was shown to be insensitive to prior-data conflict by \cite{Walter2009a},
who proposed parameter sets $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$ instead.

%*** lower, upper $\yn$ \& range for QdC, rectangle, discuss some inference properties.
Indeed, with $\PZ = \nz \times [\yzl, \yzu]$, we have $\PN = \nn \times [\ynl, \ynu]$, where
\begin{align*}
\ynl &= \frac{\nz\yzl + s}{\nz + n} & &\text{and} &
\ynu &= \frac{\nz\yzu + s}{\nz + n}\,.
\end{align*}
The posterior imprecision in the $y$ dimension, denoted by $\Delta_y(\PN)$, is 
\begin{align*}
%\label{eq:deltay}
\Delta_y(\PN) &= \ynu - \ynl = \frac{\nz (\yzu - \yzl)}{\nz +n}\,,
\end{align*}
and so the same for any fixed $n$, independent of $s$.
In contrast, parameter sets $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$
provide prior-data conflict sensitivity, since
%\footnote{See \cite{Walter2009a} or \cite[\S 3.1.4]{2013:diss-gw} for more details.}
\begin{align*}
%\label{eq:deltay2}
\Delta_y(\PN) &= \frac{\nzu (\yzu - \yzl)}{\nzu + n} %\nonumber\\
               + \inf_{\yz \in [\yzl,\yzu]} |s/n - \yz| \frac{n (\nzu - \nzl)}{(\nzl + n)(\nzu + n)}\,.
\end{align*}

As discussed at the end of \cite[\S 3.1.4]{2013:diss-gw},
the shape of $\PZ$ thus poses a trade-off:
Less complex shapes are easy to handle and lead to tractable models,
but will offer less flexibility in expressing prior information
and may have undesired inference properties.
In contrast, more complex shades may allow for more sophisticated model behaviour
at the cost of more complex handling.


\section{A Third Parametrization for Beta Priors}
\label{sec:miksworld}

In the parametrization in terms of $\nz$ and $\yz$ as described in Section~\ref{sec:beta-binom},
a conjugate Beta prior is updated to its respective posterior by a shift in the parameter space,
given by rewriting \eqref{eq:nyupdate}:
\begin{align*}
\nz &\mapsto \nz + n\,, &
\yz &\mapsto %\frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n} = 
             \yz + \frac{s - n \yz}{\nz+n}\,.
\end{align*}
We see thus that, while the shift for the $n$ coordinate is the same for all elements $(\nz,\yz)$ of $\PZ$,
the shift in the $y$ coordinate depends on $\nz$, $n$, $s$, and the location of $\yz$ itself
(namely, how far $\yz$ is from $s/n$).
Due to this, the shape of $\PZ$ changes during the update step.
This shape change from $\PZ$ to $\PN$ is problematic for understanding of the update step in generalized Bayesian inference,
as its effects on posterior inferences are difficult to grasp.

Therefore, to isolate the influence of a set shape,
we now consider a different parametrisation of the prior
where each coordinate has the same shift in updating.
The updating of a parameter set
then corresponds to a shift of the entire set within the parameter space.
This parametrization has recently been developed by Bickis \cite{2015:mik-isipta}. %Mi\c{k}elis Bickis
In this novel parametrization, a prior is represented by a coordinate $(\ezz,\eoz)$,
where $\eoz$ replaces $\yz$, while $\ezz$ is just a shifted version of $\nz$.
%there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 
To keep things simple, we will not describe its derivation here (this is described in detail in \cite{2015:mik-isipta}),
and present it in relation to $(\nz, \yz)$:
%\begin{equation}
\begin{align}
\label{eq:trafotony}
%\begin{aligned}
\nz &= \ezz + 2\,, &
\yz &= \frac{\eoz}{\ezz + 2} + \frac{1}{2}\,.
%\end{aligned}
%\end{equation}
\end{align}
The domain of $\eta_0$ and $\eta_1$ in case of the Beta-Binomial model is
\begin{align}
\label{eq:eta-domain}
\Eta &= \Big\{ (\eta_0,\eta_1) \Big| \eta_0 > -2,\ |\eta_1| < \frac{1}{2}(\eta_0 + 2) \Big.\Big\}\,,
\end{align}
and the update step in terms of $\eta_0$ and $\eta_1$ is given by
\begin{equation}
\label{eq:eta-update}
\begin{aligned}
\eta_0\un &= \eta_0\uz + n\,, & 
\eta_1\un &= \eta_1\uz + \frac{1}{2}(s - (n-s)) = \eta_1\uz + s - \frac{n}{2}\,.
\end{aligned}
\end{equation}
%where $s$ is again the number of successes in the $n$ Bernoulli trials.
Each success thus
leads to a step of $1$ in the $\eta_0$ direction and of $+\frac{1}{2}$ in the $\eta_1$ direction,
while each failure
leads to a step of $1$ in the $\eta_0$ direction and of $-\frac{1}{2}$ in the $\eta_1$ direction. %
%\footnote{We will treat $s$ as a a real-valued observation in $[0,n]$
%because the continuous representation is convenient for our discussions,
%keeping in mind that in reality it can only take integer values.}
%\begin{figure}  %trim=l b r t
%\centering
%\fbox{%
%\includegraphics[trim = 86mm 48mm 94mm 62mm, clip, width=0.5\textwidth]{R/boatshape-domain}%
%}
%\caption[Bounds for the domain of $\eta_0$ and $\eta_1$ for the Beta-Binomial model,
%with rays of constant expectation for $y_c = \{0.1,0.2,\ldots,0.9\}$.]%
%{Bounds for the domain of $\eta_0$ and $\eta_1$ for the Beta-Binomial model (black),
%with rays of constant expectation for $y_c = \{0.1,0.2,\ldots,0.9\}$ (grey).}
%\label{fig:boatshape-domain}
%\end{figure}

While $\yz$ had the convenient property of being equal to
the prior expectation of the mean sample statistic $s/n$,
$\eta_1$ is only slightly more difficult to interpret.
From \eqref{eq:trafotony} we can derive that points $(\eta_0,\eta_1) \in \Eta$
on rays emanating from the coordinate $(-2,0)$,
i.e., coordinates satifying
\begin{align}
\label{eq:raysofconstantexpectation}
\eta_1 &= (\eta_0 + 2)(y_c - 1/2) 
\end{align}
will have a constant expectation of $y_c$.
The domain $\Eta$, and these \emph{rays of constant expectation} emanating from the coordinate $(-2,0)$,
can be seen in Figure~\ref{fig:boatshape-vertical}.

In the parametrisation in terms of $(\nz,\yz)$,
posterior inferences based on $\yn$ become more precise with increasing $n$
because $\Delta_y(\PN) \to 0$ for $n \to \infty$.
In the domain $\Eta$, %as depicted in Figure~\ref{fig:boatshape-domain},
instead the rays of constant expectation fan out for growing $n$,
while a parameter set will retain its size in updating.
Increased precision in a posterior parameter set $\EN$, which is just
its prior counterpart $\EZ$ shifted to the right,
is given by the fact that the more $\EN$ is located to the right,
the fewer rays of constant expectation $\EN$ it will intercept.
Imprecision in terms of $\yn$
can thus be imagined as the size of the `shadow' that a set $\EN$ casts
when considering a light source in $(-2,0)$. %(the point from which the rays of constant expectation emanate).
The smaller this shadow, the more precise the inferences.
In terms of the $(\nz, \yz)$ parametrization, we will denote the bounds of this shadow by $\ynl$ and $\ynu$:
\begin{align*}
\ynl &:= \min_{(\ezn,\eon) \in \EN} \frac{\eon}{\ezn+2} + \frac{1}{2}\,, &
\ynu &:= \max_{(\ezn,\eon) \in \EN} \frac{\eon}{\ezn+2} + \frac{1}{2}\,.
\end{align*}
We call the $\ez$ coordinate of $\arg\min_{(\eta_0,\eta_1) \in \EN} \yn$ and $\arg\max_{(\eta_0,\eta_1) \in \EN} \yn$
the \emph{lower} and \emph{upper touchpoint} of $\EN$ responsible for the shadow $[\ynl, \ynu]$.
Mutatis mutandis, the same definitions can be made for the prior set $\EZ$.

Due to the fanning out of rays, most shapes for $\EZ$ will lead to decreasing imprecision for increasing $n$.
Indeed, models with $\PZ = \nz \times [\yzl, \yzu]$
are represented here again by a line segment $\EZ = \ezz \times [\eozl,\eozu]$.
%such that the lower and upper posterior touchpoint is at $\ezn$, for any $s$ and $n$.
Defining $\eonl$ and $\eonu$ as the updated versions of $\eozl$ and $\eozu$, respectively,
we see that $\eonu-\eonl = \eozu-\eozl$;
therefore, imprecision decreases here because a line segment of fixed size
will cast a smaller shadow when further to the right,
as illustrated in Figure~\ref{fig:boatshape-vertical}.
\begin{figure}  %trim=l b r t
\centering
%\fbox{%
\includegraphics[trim = 16mm 48mm 25mm 62mm, clip, width=\textwidth]{R/boatshape-vertical}%
%}
\caption[Line segment parameter set $\EZ$ %
and respective posterior sets for $s/n=0.5$ and $s/n=0.9$.]%
%{Bounds for the domain of $\eta_0$ and $\eta_1$ for the Beta-Binomial model (black),
%with rays of constant expectation for $y_c = \{0.1,0.2,\ldots,0.9\}$ (grey).}
{Prior parameter set $\EZ = \ezz \times [\text{\underline{$\eta$}${}_1\uz$},\eozu]$
and respective posterior sets $\EN$ for $s/n=0.5$ (left) and $s/n=0.9$ (right).
Bounds for the domain $\Eta$ are in black, with rays of constant expectation for $y_c = \{0.1,0.2,\ldots,0.9\}$ in grey. 
Note that all sets have the same size, imprecision decreasing only through their position on the $\eta_0$ axis.}
\label{fig:boatshape-vertical}
\end{figure}

For \pdc\ sensitivity, we need shapes that cover a range of $\eta_0$ values,
just like we needed sets with a range of $\nz$ values to ensure this property.
Sets that are elongated along a certain ray of constant expectation
will behave here similar to the rectangular shapes of Section~\ref{sec:setsofbetapriors}.
When shifted along its ray of constant expectation,
imprecision will be reduced as the shadow of the set will become smaller just as described above for line segments.
When such a shape is instead shifted away from its ray of constant expectation,
imprecision will increase, as a prolonged shape that is now turned away from its ray 
will cast a larger shadow.%
\footnote{This will become clear from the depiction of boatshape sets in Figure~\ref{fig:boatshape-posterior-mik}.} 
%A set $\EZ$ allowing for more precison in case of strong prior-data agreement
%must thus be able to cast a smaller shadow if the update shift is along the direction of its ray.
%of $s/n$ according the information,


\section{The Boatshape}
\label{sec:boatshape}

The shape for $\EZ$ that we suggest to obtain both \pdc\ sensitivity and increased precision
in case of strong prior-data agreement looks like a bullett, or like a boat with a transom stern
(see, e.g., Figure~\ref{fig:boatshape-posterior-mik} below), hence its name.
The curvature along its length in the direction of its constant rays of expectation
leads to smaller $\Delta_y(\PN)$ as compared to a rectangular $\PZ$ with the same prior range $\Delta_y(\PZ)$,
as illustrated in Figure~\ref{fig:boatshape-posterior-normal}.

The strong prior-data agreement effect is realized
through the touchpoints determining $\ynl$ and $\ynu$
moving along the shape during updating,
%being attained at higher values of $\eta_0$ for larger $n$,
as will be demonstrated in Section~\ref{sec:spda-property}.
This is advantageous when considering that the spread of the Beta posteriors is determined by $\eta_0 = \nz - 2$.
In case of strong prior-data agreement, variances in the `critical' distributions
at the boundary of the posterior expectation interval $[\ynl,\ynu]$ will thus be lower.
In particular, credible intervals for these `critical' distributions will be shorter, 
such that the union of all credible intervals for Beta distributions with $(\ezn, \eon) \in \EN$,
taken as a posterior inference reflecting uncertainty due to both vague prior knowledge and stochasticity, will be shorter.

In the remainder of this section, 
we will suggest a parametrisation for a curved shape in the $(\eta_0, \eta_1)$ space.
The definition, along with some graphical examples, is given in Section~\ref{sec:basicdefboat},
and we discuss a number of technical results for this shape in subsequent sections.
%Sections~\ref{sec:touchpoints} -- \ref{sec:generalupdate}.


\subsection{Basic Definition}
\label{sec:basicdefboat}

We will now present a parametrisation for such a boat-shaped parameter set $\EZ$.
To keep things simple, we will consider at first only prior sets
that are symmetric around the $\eta_0$ axis, i.e., centered around $y_c = 0.5$,
expressing the prior information that we deem a fraction of successes of $\frac{s}{n} = \frac{1}{2}$
as the most probable. Sets $\EZ$ with central ray $y_c \neq 0.5$
will be discussed in Section~\ref{}.****

For the contours of $\EZ$, we suggest an exponential function as the functional form,
where the `prow' of the set is located at $(\ezl, 0)$.
The lower and the upper contour $\czl(\eta_0)$ and $\czu(\eta_0)$ are defined as
%\begin{align*}
%\czl(\eta_0) &= -a \left( 1 - e^{-b(\eta_0 - \ezl)} \right)\,, \\
%\czu(\eta_0) &= \phantom{-}%
%                 a \left( 1 - e^{-b(\eta_0 - \ezl)} \right)\,, 
%\end{align*}
\begin{align}
\czl(\eta_0) &= -a \left( 1 - e^{-b(\eta_0 - \ezl)} \right)\,, &
\czu(\eta_0) &=  a \left( 1 - e^{-b(\eta_0 - \ezl)} \right)\,, 
\label{eq:basiccontours}
\end{align}
where $a$ and $b$ are parameters controlling the shape of $\EZ$.
%We will also need the respective derivations with respect to $\eta_0$, given by
%\begin{align*}
%\frac{d}{d\eta_0} \czl(\eta_0) &= -ab e^{-b(\eta_0 - \ezl)}\,, \\
%\frac{d}{d\eta_0} \czu(\eta_0) &= \phantom{-}%
%                                   ab e^{-b(\eta_0 - \ezl)}\,.
%\end{align*}
Given parameters $\ezl$, $\ezu$, $a$, and $b$, $\EZ$ is thus defined as
\begin{align}
\label{eq:basicset}
\EZ =
\{(\eta_0,\eta_1) \colon \ezl \le \eta_0 \le \ezu, \czl(\eta_0) \le  \eta_1 \le \czu(\eta_0) \}\,.
\end{align}
A prior boatshape set with $\ezl=1$, $\ezu=6$, $a=1.5$, and $b=0.9$,
together with corresponding posterior sets for different observations,
is depicted in Figure~\ref{fig:boatshape-posterior-mik}.
The same prior and posterior sets in terms of $(\nz, \yz)$ are depicted in Fig.~\ref{fig:boatshape-posterior-normal}.
%shown in terms of $(\eta_0,\eta_1)$ (left) and in terms of $(\nz, \yz)$ (right).
%\begin{figure}  %trim=l b r t
%\centering
%\fbox{%
%\includegraphics[trim = 15mm 45mm 25mm 60mm, clip, width=\textwidth]{R/boatshape-prior}%
%}
%\caption[Boatshape prior set in the parametrisation via $(\eta_0,\eta_1)$ and via $(\nz,\yz)$.]%
%{Boatshape prior set in the parametrisation via $(\eta_0,\eta_1)$ (left) and via $(\nz,\yz)$ (right),
%with parameters $\ezl=1$, $\ezu=6$, $a=2$, and $b=0.8$.}
%\label{fig:boatshape-prior}
%\end{figure}
\begin{figure}  %trim=l b r t
\centering
%\fbox{%
%\includegraphics[trim = 20mm 35mm 30mm 45mm, clip, width=\textwidth]{R/boatshape-posterior-mik}%
\includegraphics[width=\textwidth]{R/boatshape-posterior-mik}%
%}
\caption[Boatshape prior and posterior sets for data in accordance and in conflict with the prior set.]%
{Boatshape prior and posterior sets for data in accordance and in conflict with the prior set.
The parameters for the prior set are $\ezl=1$, $\ezu=6$, $a=1.5$, and $b=0.9$.
While the posterior sets for $\frac{s}{n}=0.5$ move along the ray for $y_c=0.5$,
the posterior sets for $\frac{s}{n}=1$ are shifted away from the ray for $y_c=0.5$,
resulting in increased posterior imprecision.
Note that lower and upper touchpoints are in the middle of the contour
for the prior set and the posterior sets resulting for data $\frac{s}{n}=0.5$,
while the lower touchpoint is at the end for the posterior sets for data $\frac{s}{n}=1$.}
%(See also Figure~\ref{fig:boatshape-posterior-normal}).}
\label{fig:boatshape-posterior-mik}
\end{figure}

We have as yet no appealing formal description for the role of the parameters $a$ and $b$. %that,
%togeher with $\ezl$ and $\ezu$, define the boat-set (see Section~\ref{sec:basicdefboat} below).
Informally, $a$ determines the half-width of the set;
the width, i.e., the size in the $\eta_1$ dimension, would be $a$ if $\ezu \to \infty$.
$b$ instead determines the `bulkyness' of the shape.
Together with $\ezl$, $a$ and $b$ determine the prior interval for the expected success probability $[\yzl, \yzu]$.
Increasing each of $\ezl$, $a$ and $b$ while keeping the other two fixed leads to a wider prior interval $[\yzl, \yzu]$.
%For fixed $\ezl$ and $a$, increasing $b$ leads to a wider prior expectation interval.
%For $[\yzl, \yzu]$, the choice of $\ezu$ is irrelevant.%
$\ezu$ plays only a role in determining when the `unhappy learning' phase starts
(see end of Section~\ref{sec:generalupdate}).
\begin{figure}  %trim=l b r t
\centering
%\fbox{%
%\includegraphics[trim = 15mm 45mm 25mm 60mm, clip, width=\textwidth]{R/boatshape-posterior-normal}%
\includegraphics[width=\textwidth]{R/boatshape-posterior-normal}%
%}
\caption[Boatshape prior and posterior sets from Figure~\ref{fig:boatshape-posterior-mik} in the $(\nz,\yz)$ parametrisation.]%
{Boatshape prior and posterior sets from Figure~\ref{fig:boatshape-posterior-mik} in the $(\nz,\yz)$ parametrisation.
The rectangular prior set with the same range for $\yz$ as the prior boatshape set
and the corresponding posterior sets are drawn with dashed lines. 
Note that all posterior boatshape sets have shorter $\yn$ ranges than their corresponding posterior rectangle sets.}
\label{fig:boatshape-posterior-normal}
\end{figure}

We see from the prior set in Figure~\ref{fig:boatshape-posterior-normal}
that the lower and the upper bound for $\yz$
is attained in the middle of the set contour.
To determine $\yzl$ and $\yzu$, we thus need to find the corresponding
touchpoints ${\eta_0^l}\uz$ and ${\eta_0^u}\uz$.
This can be done by identifying the rays of constant expectation \eqref{eq:raysofconstantexpectation}
that are tangents to $\EZ$ and then solving for $\eta_0$.
With $\EZ$ being symmetric to the $\eta_0$ axis, we have ${\eta_0^u}\uz = {\eta_0^l}\uz$,
so we will determine ${\eta_0^u}\uz$ only by considering the upper contour tangent.
We get
\begin{align}
%a - a \big(1 + b(\eta_0^u + 2)\big) e^{-b(\eta_0^u - \ezl)} &\stackrel{!}{=} 0
%& &\Longleftrightarrow &
1 + b({\eta_0^u}\uz + 2) &\stackrel{!}{=} e^{b({\eta_0^u}\uz - \ezl)}\,.
\label{eq:eta0uprior}
\end{align}
This equation has only one solution for ${\eta_0^u}\uz > \ezl$
that is, however, not available in closed form.
Generally, the nearer ${\eta_0^u}\uz$ is to $\ezl$, the larger $\frac{d}{d\eta_0} \czu({\eta_0^u}\uz)$,
such that $\yzu$ %the upper expected value for the prior set
is further away from $\frac{1}{2}$.
This means that, for fixed $\ezl$, the smaller ${\eta_0^u}\uz$, the more imprecise the prior parameter set.


\subsection{Strong Prior-Data Agreement Property}
\label{sec:spda-property}

We will now derive the essential property that sets \eqref{eq:basicset}
will lead to especially precise inferences when data are strongly supporting prior information,
since the touchpoint moves further to the right in that case. %when updating with strong-agreement data.
The basic shape is symmetric around the $\ez$ axis ($\EZ$ has central ray $y_c = 0.5$),
and update with strong-agreement data $s/n = 0.5$ means that $\EZ$ is shifted along the $\ez$ axis by $n$,
such that also $\EN$ is symmetric around the $\ez$ axis.
We thus need to consider only one of the two touchpoints.
With the upper prior touchpoint being at ${\eta_0^u}\uz$,
the touchpoint movement to the right means that the upper posterior touchpoint ${\eta_0^u}\un$
is larger than the updated prior touchpoint,
so we need to show that ${\eta_0^u}\un > {\eta_0^u}\uz + n$.

The upper contour for the posterior boatshape,
updated with $s = \frac{n}{2}$, %has its `prow' now at $(\ezl + n, 0)$,
is $\czu$ from \eqref{eq:basiccontours} shifted to the right by $n$,
\begin{align*}
\cnu(\eta_0) &= a \left( 1 - e^{-b(\eta_0 - n -\ezl)} \right)\,.
%\frac{d}{d\eta_0} \ol{c}(\eta_0) &= ab e^{-b(\eta_0 - n - \ezl)} \,.
\end{align*}
The equation to identify the posterior upper touchpoint is then
\begin{align}
%a - a \big(1 + b({\eta_0^u}\un + 2)\big) e^{-b({\eta_0^u}\un - n - \ezl)} &\stackrel{!}{=} 0 \nonumber\\
1 + b({\eta_0^u}\un + 2) &\stackrel{!}{=} e^{b({\eta_0^u}\un - n - \ezl)} \,.
\label{eq:eta0uposterior}
\end{align}
We compare now \eqref{eq:eta0uposterior} to \eqref{eq:eta0uprior} and conclude
that indeed ${\eta_0^u}\un > {\eta_0^u}\uz + n$:


%\footnote{We will treat $s$ as a a real-valued observation in $[0,n]$
%because the continuous representation is convenient for our discussions,
%keeping in mind that in reality it can only take integer values.}

\section{Discussion and Concluding Remarks}
\label{sec:concluding}


% ------------ bibliography -------------

\printbibliography


\end{document}
